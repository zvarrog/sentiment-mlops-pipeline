import pandas as pd
import pytest

from scripts.feature_engineering import (
    clean_text,
    extract_basic_text_features,
    transform_features,
)


def test_clean_text_logic():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—á–∏—Å—Ç–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç—Å—è (lower + remove punctuation)."""
    raw = "HELLO World!!! http://link.com"
    cleaned = clean_text(raw)
    assert cleaned == "hello world"


def test_features_on_raw_text():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –ø–æ –°–´–†–û–ú–£ —Ç–µ–∫—Å—Ç—É (–¥–æ –æ—á–∏—Å—Ç–∫–∏)."""
    raw = "HELLO World!!!"
    # –ï—Å–ª–∏ –±—ã —Å—á–∏—Ç–∞–ª–∏ –ø–æ —á–∏—Å—Ç–æ–º—É ("hello world"), —Ç–æ caps_ratio=0, exclamation=0

    df_raw = pd.Series([raw])
    features = extract_basic_text_features(df_raw)

    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    assert features.iloc[0]["exclamation_count"] == 3.0
    assert features.iloc[0]["caps_ratio"] > 0.0
    assert features.iloc[0]["text_len"] == len(raw)


def test_transform_features_integration():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é transform_features."""
    texts = ["HELLO World!!!", "simple text"]
    df, ignored = transform_features(texts, numeric_features=None, expected_numeric_cols=[])

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ df –µ—Å—Ç—å –∏ –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ–∏—á–∏
    assert df.iloc[0]["reviewText"] == "hello world"
    assert df.iloc[0]["exclamation_count"] == 3.0
    assert df.iloc[0]["caps_ratio"] > 0.0

    assert df.iloc[1]["reviewText"] == "simple text"
    assert df.iloc[1]["exclamation_count"] == 0.0
    assert df.iloc[1]["caps_ratio"] == 0.0


# ============================================================
# Edge Cases
# ============================================================


class TestEdgeCases:
    """–ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è feature engineering."""

    def test_empty_string(self):
        """–ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –ª–æ–º–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω."""
        result = clean_text("")
        assert result == ""

        features = extract_basic_text_features(pd.Series([""]))
        assert features.iloc[0]["text_len"] == 0.0
        assert features.iloc[0]["word_count"] == 0.0

    def test_none_value(self):
        """None –¥–æ–ª–∂–µ–Ω –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –∫–∞–∫ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞."""
        result = clean_text(None)
        assert result == ""

    def test_whitespace_only(self):
        """–°—Ç—Ä–æ–∫–∞ —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤."""
        result = clean_text("   \t\n  ")
        assert result == ""

        features = extract_basic_text_features(pd.Series(["   "]))
        assert features.iloc[0]["word_count"] == 0.0

    def test_unicode_emoji(self):
        """Unicode –∏ emoji –Ω–µ –¥–æ–ª–∂–Ω—ã –ª–æ–º–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É."""
        text = "Great product! üî•üî• –û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä! –¶–µ–Ωa: ‚Ç¨100"
        result = clean_text(text)
        # –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –ª–∞—Ç–∏–Ω–∏—Ü—É –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        assert "great" in result
        assert "product" in result
        # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã —É–¥–∞–ª—è—é—Ç—Å—è
        assert "üî•" not in result
        assert "‚Ç¨" not in result

    def test_very_long_text(self):
        """–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (10KB+)."""
        long_text = "word " * 5000  # ~25KB
        result = clean_text(long_text)
        assert len(result) > 0

        features = extract_basic_text_features(pd.Series([long_text]))
        assert features.iloc[0]["word_count"] == 5000.0

    def test_special_characters_only(self):
        """–¢–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∏–∑ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤."""
        text = "!@#$%^&*()_+-=[]{}|;':\",./<>?"
        result = clean_text(text)
        assert result == ""

        features = extract_basic_text_features(pd.Series([text]))
        assert features.iloc[0]["exclamation_count"] == 1.0
        assert features.iloc[0]["question_count"] == 1.0

    def test_mixed_case_consistency(self):
        """–†–µ–≥–∏—Å—Ç—Ä –¥–æ–ª–∂–µ–Ω –≤–ª–∏—è—Ç—å –Ω–∞ caps_ratio, –Ω–æ –Ω–µ –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç."""
        lower = "hello world"
        upper = "HELLO WORLD"
        mixed = "HeLLo WoRLd"

        assert clean_text(lower) == clean_text(upper) == clean_text(mixed)

        features_lower = extract_basic_text_features(pd.Series([lower]))
        features_upper = extract_basic_text_features(pd.Series([upper]))

        assert features_lower.iloc[0]["caps_ratio"] == 0.0
        assert features_upper.iloc[0]["caps_ratio"] > 0.5

    def test_numbers_and_urls(self):
        """–ß–∏—Å–ª–∞ –∏ URL –¥–æ–ª–∂–Ω—ã —É–¥–∞–ª—è—Ç—å—Å—è."""
        text = "Check http://example.com and call 123-456-7890"
        result = clean_text(text)
        assert "http" not in result
        assert "123" not in result
        assert "check" in result
        assert "call" in result

    @pytest.mark.parametrize(
        "text,expected_exclaim,expected_question",
        [
            ("Hello!", 1.0, 0.0),
            ("What?", 0.0, 1.0),
            ("Really?!", 1.0, 1.0),
            ("Wow!!! Amazing!!!", 6.0, 0.0),
            ("???", 0.0, 3.0),
        ],
    )
    def test_punctuation_counting(self, text, expected_exclaim, expected_question):
        """–ü–æ–¥—Å—á—ë—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ—á–Ω—ã–º."""
        features = extract_basic_text_features(pd.Series([text]))
        assert features.iloc[0]["exclamation_count"] == expected_exclaim
        assert features.iloc[0]["question_count"] == expected_question
